# Feature Selection

* Handling Missing Values:

```python
import pandas as pd
data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]

# These are the columns that will be dropped.
columns = ['PID', 'Year Built', 'Year Remod/Add',
           'Garage Yr Blt', 'Mo Sold', 'Yr Sold']

# This syntax helps me select just the integer and float columns
numerical_train = train.select_dtypes(include = ['int', 'float'])
print(numerical_train.shape)

numerical_train = numerical_train.drop(columns = columns)
print(numerical_train.shape)

# Running this code will calculate the number of missin values from each column.
null_series = numerical_train.isnull().sum()

# This sill subset to keep only the columns with no missing values.
full_cols_series = null_series[null_series == 0]
```

* Correlating Feature Colummns With Target Column:

```python
train_subset = train[full_cols_series.index]

# This nested line of code will correlate, return the absolute value and subset by 'SalePrice'
sorted_corrs = train_subset.corr().abs().loc['SalePrice'].sort_values()
```

* Correlation Matrix Heatmap:

![Heatmap1](https://s3.amazonaws.com/dq-content/236/correlation_heatmap_matrix.png)

```python
import seaborn as sns
import matplotlib.pyplot as plt

strong_corrs = sorted_corrs[sorted_corrs > 0.3]
corrmat = train_subset[strong_corrs.index].corr()
sns.heatmap(corrmat)
```

* Train and Test Model:

```python
>>> final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])
>>> test[final_corr_cols.index].info()
class 'pandas.core.frame.DataFrame'
RangeIndex: 1470 entries, 1460 to 2929
Data columns (total 9 columns):
Wood Deck SF     1470 non-null int64
Open Porch SF    1470 non-null int64
Fireplaces       1470 non-null int64
Full Bath        1470 non-null int64
1st Flr SF       1470 non-null int64
Garage Area      1469 non-null float64
Gr Liv Area      1470 non-null int64
Overall Qual     1470 non-null int64
SalePrice        1470 non-null int64
dtypes: float64(1), int64(8)
memory usage: 103.4 KB
```

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Data Preparation.
final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])
features = final_corr_cols.drop(['SalePrice']).index
target = 'SalePrice'
clean_test = test[final_corr_cols.index].dropna()

# Machine Learning.
lr = LinearRegression()
lr.fit(train[features], train[target])
train_predictions = lr.predict(train[features])
test_predictions = lr.predict(clean_test[features])

# Calculating Error.
train_rmse = np.sqrt(mean_squared_error(train_predictions, train[target]))
test_rmse = np.sqrt(mean_squared_error(test_predictions, clean_test[target]))
```

* Removing Low Variance Features:

```python
def rescaling(series):
    value = (series - series.min()) / (series.max() - series.min())
    return value

sorted_vars = train[features].apply(rescaling)

print(sorted_vars.max())
print(sorted_vars.min())

sorted_vars = sorted_vars.var().sort_values()
print(sorted_vars)
```

# Setting cutoff variance of 0.015:

```python
# Dropping column with variance lower than 0.015
features = features.drop("Open Porch SF")

# Machine Learning:
lr = LinearRegression()
lr.fit(train[features], train[target])
train_pre = lr.predict(train[features])
test_pre = lr.predict(clean_test[features])

# Error Calculation:
train_rmse_2 = np.sqrt(mean_squared_error(train_pre, train[target]))
test_rmse_2 = np.sqrt(mean_squared_error(test_pre, clean_test[target]))  
```
