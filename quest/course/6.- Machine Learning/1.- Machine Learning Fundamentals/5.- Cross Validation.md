# Cross Validation

* Holdout validation technique:
  * Splitting the full dataset into 2 partitions:
    * A training set (50%).
    * A test set (50%).
  * Training the model on the training set.
  * Using the trained model to predict labels on the test set.
  * Computing an error metric to understand the model's effectiveness.
  * Switch the training and test sets and repeat.
  * Average the errors
  
![Holdout1](https://s3.amazonaws.com/dq-content/holdout_validation.png)

* Splitting data:

```python
import numpy as np
import pandas as pd

dc_listings = pd.read_csv("dc_airbnb.csv")
stripped_commas = dc_listings['price'].str.replace(',', '')
stripped_dollars = stripped_commas.str.replace('$', '')
dc_listings['price'] = stripped_dollars.astype('float')
dc_listings = dc_listings.loc[numpy.random.permutation(len(dc_listings))]

split_one = dc_listings.iloc[:1862].copy()
split_two = dc_listings.iloc[1862:].copy()
```

* Holdout Validation:

```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

train_one = split_one
test_one = split_two
train_two = split_two
test_two = split_one

knn = KNeighborsRegressor(algorithm = "auto", n_neighbors = 5)

#Features must come as a list, not a column: in this case we must use [["accomodates"]]

knn.fit(train_one[["accommodates"]], train_one["price"])
prediction = knn.predict(test_one[["accommodates"]])
iteration_one_rmse = np.sqrt(mean_squared_error(test_one["price"], prediction))

knn = KNeighborsRegressor(algorithm = "auto", n_neighbors = 5)
knn.fit(train_two[["accommodates"]], train_two["price"])
prediction = knn.predict(test_two[["accommodates"]])
iteration_two_rmse = np.sqrt(mean_squared_error(test_two["price"], prediction))

avg_rmse = np.mean([iteration_one_rmse, iteration_two_rmse])
```

* K-Fold Cross Validation:

![K-fold1](https://s3.amazonaws.com/dq-content/kfold_cross_validation.png)

```python
dc_listings.loc[dc_listings.index[0:745], "fold"] = 1
dc_listings.loc[dc_listings.index[745:1490], "fold"] = 2
dc_listings.loc[dc_listings.index[1490:2234], "fold"] = 3
dc_listings.loc[dc_listings.index[2234:2978], "fold"] = 4
dc_listings.loc[dc_listings.index[2978:3723], "fold"] = 5

print(dc_listings['fold'].value_counts().sort_index())
print("\n Num of missing values: ", dc_listings['fold'].isnull().sum())
```

* First iteration:

```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor()

train = dc_listings[dc_listings["fold"] != 1].copy()
test = dc_listings[dc_listings["fold"] == 1].copy()

knn.fit(train[["accommodates"]], train["price"])
labels = knn.predict(test[["accommodates"]])
iteration_one_rmse = np.sqrt(mean_squared_error(test["price"], labels))
```

* Function for training models:

```python
import numpy as np

fold_ids = [1,2,3,4,5]

def train_and_validate(df, anylist):
    rmses = []
    
    for i in anylist:
        knn = KNeighborsRegressor()
        train = dc_listings[dc_listings["fold"] != i].copy()
        test = dc_listings[dc_listings["fold"] == i].copy()
        knn.fit(train[["accommodates"]], train["price"])
        labels = knn.predict(test[["accommodates"]])
        rmses.append(np.sqrt(mean_squared_error(test["price"], labels)))
    
    return rmses, np.mean(rmses)

rmses, avg_rmse = train_and_validate(dc_listings, fold_ids)
```

* Performing K-Fold Cross Validation Using Scikit-Learn:

  Instantiate an instance of the Kfold class from sklearn.model_selection:
  
  ```python
  from sklearn.model_selection import KFold
  kf = KFold(n_splits, shuffle=False, random_state=None)
  ```
  
  where:
  
    * `n_splits`: is the number of folds you want to use.
    * `shuffle`: is used to toggle shuffling of the ordering of the observations in the dataset.
    * `random_state`: is used to specify the random seed value if `shuffle` is set to `True`.
    
  Using the `cross_val_score` function:
  
  ```python
  from sklearn.model_selection import cross_val_score
  cross_val_score(estimator, X, Y, scoring=None, cv=None)
  ```
  
  where:
  
    * `estimator`: is a sklean model that implements the `fit` method.
    * `X` is the list or 2d array containing the features you want to train on.
    * `Y` is a list containing the values you wat to predict (target column).
    * `scoring` is a string describing the scoring criteria.
    * `cv` describes the number of folds such as:
      * an instance of the `Kfold` class.
      * an integer representing the number of folds.
      
  * instantiate the scikit-learn model class you want to fit.
  * instantiate the `Kfold` class and using the parameters to specify the k-fold cross validation attributes.
  * use the `cross_val_score()` function to return the scoring metric.
  
```python
from sklearn.model_selection import cross_val_score, KFold

kf = KFold(5, shuffle = True, random_state = 1)
knn = KNeighborsRegressor()

mses = cross_val_score(knn, dc_listings[["accommodates"]], dc_listings["price"],
                scoring = "neg_mean_squared_error", cv = kf)

avg_rmse = np.mean(np.sqrt(np.absolute(mses)))
```

* Exploring Different K Values:

```python
from sklearn.model_selection import cross_val_score, KFold

num_folds = [3, 5, 7, 9, 10, 11, 13, 15, 17, 19, 21, 23]

for fold in num_folds:
    kf = KFold(fold, shuffle=True, random_state=1)
    model = KNeighborsRegressor()
    mses = cross_val_score(model, dc_listings[["accommodates"]], dc_listings["price"], scoring="neg_mean_squared_error", cv=kf)
    rmses = np.sqrt(np.absolute(mses))
    avg_rmse = np.mean(rmses)
    std_rmse = np.std(rmses)
    print(str(fold), "folds: ", "avg RMSE: ", str(avg_rmse), "std RMSE: ", str(std_rmse))
```

* Bias-Variance Tradeoff:

![Bias-Variance](https://s3.amazonaws.com/dq-content/bias_variance.png)

Standard deviation of RMSE is a proxy for variance.
Mean of RMSE is a proxy for bias.

Bias describes errors that results in bad assumptions about the learning algorithm.
Variance describes error that occurs because of the variability of a moidel's predicted values.
