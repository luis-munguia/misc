# Hyperparameter Optimization

* Hyperparameter optimization:

Use the grid search technique:

  * Selecting a subset of the possible hyperparameter values.
  * Training a model using each of these hyperparameters values.
  * Evaluating each model's performance.
  * Selecting the hyperparameter value that resulted in the lowest error value.
  
```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

hyper_params = [1,2,3,4,5]
features = ["accommodates", "bedrooms", "bathrooms", "number_of_reviews"]
mse_values = []

for i in hyper_params:
    knn = KNeighborsRegressor(n_neighbors = i, algorithm = "brute")
    knn.fit(train_df[features], train_df["price"])
    predictions = knn.predict(test_df[features])
    mse_values.append(mean_squared_error(test_df["price"], predictions))

print(mse_values) 
```

* Expanding grid search:

```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

hyper_params = range(1,21)
# Also can use hyper_params = [x for x in range(1,21)]
features = ["accommodates", "bedrooms", "bathrooms", "number_of_reviews"]
mse_values = []

for i in hyper_params:
    knn = KNeighborsRegressor(n_neighbors = i, algorithm = "brute")
    knn.fit(train_df[features], train_df["price"])
    predictions = knn.predict(test_df[features])
    mse_values.append(mean_squared_error(test_df["price"], predictions))

print(mse_values)
```

|k|MSE|
|---|---|
|1|26364.928327645051|
|2|15100.522468714449|
|3|14579.597901655923|
|4|16212.300767918088|
|5|14090.011649601822|
|6|13657.290671217292|
|7|14288.273896589353|
|8|14853.448183304892|
|9|14670.831907751512|
|10|14642.451478953355|
|11+|14771.124646694478|

The best value was 6.

* Visualizing hyperparameter values:

```python
import matplotlib.pyplot as plt
%matplotlib inline

features = ['accommodates', 'bedrooms', 'bathrooms', 'number_of_reviews']
hyper_params = [x for x in range(1, 21)]
mse_values = list()

for hp in hyper_params:
    knn = KNeighborsRegressor(n_neighbors=hp, algorithm='brute')
    knn.fit(train_df[features], train_df['price'])
    predictions = knn.predict(test_df[features])
    mse = mean_squared_error(test_df['price'], predictions)
    mse_values.append(mse)
    
plt.scatter(hyper_params, mse_values)
plt.show()
```

* Varying features and hyperparameters:

```python
hyper_params = [x for x in range(1,21)]
mse_values = list()
all_features = list(train_df.columns)
del all_features[4]
#features = train_df.columns.tolist()
#features.remove('price')

for i in hyper_params:
    knn = KNeighborsRegressor(n_neighbors = i, algorithm = "brute")
    knn.fit(train_df[all_features], train_df["price"])
    predictions = knn.predict(test_df[all_features])
    mse_values.append(mean_squared_error(test_df["price"], predictions))
    
plt.scatter(hyper_params, mse_values)
plt.show()
```

* Workflow:

```python
two_features = ['accommodates', 'bathrooms']
three_features = ['accommodates', 'bathrooms', 'bedrooms']
hyper_params = [x for x in range(1,21)]
# Append the first model's MSE values to this list.
two_mse_values = list()
# Append the second model's MSE values to this list.
three_mse_values = list()
two_hyp_mse = dict()
three_hyp_mse = dict()

def machine_knn(parameters, features, train, test):
    temp_list = []
    temp_directory = {}
    return_dictionary = {}
    for i in parameters:
        knn = KNeighborsRegressor(n_neighbors = i, algorithm = "brute")
        knn.fit(train[features], train["price"])
        predictions = knn.predict(test_df[features])
        temp_list.append(mean_squared_error(test["price"], predictions))
        temp_directory[i] = temp_list[i-1]
        
    return_dictionary[min(temp_directory, key=temp_directory.get)] = temp_directory[min(temp_directory, key=temp_directory.get)]
    
    return temp_list, return_dictionary

two_mse_values, two_hyp_mse = machine_knn(hyper_params, two_features, train_df, test_df)
three_mse_values, three_hyp_mse = machine_knn(hyper_params, three_features, train_df, test_df)
```
